{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b01abac",
   "metadata": {},
   "source": [
    "# AWS SageMaker Data Lake and Feature Store for Time Series Forecasting\n",
    "\n",
    "This notebook creates:\n",
    "1. **Data Lake** - S3 buckets + Athena tables for raw aggregated time series data\n",
    "2. **Feature Store** - SageMaker Feature Store for engineered features\n",
    "\n",
    "This supports the demand forecasting models built in Modeling.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4102d13f",
   "metadata": {},
   "source": [
    "## Part 1: Data Lake Setup\n",
    "\n",
    "Initialize SageMaker SDK and configure AWS resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72b265a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Initialize SageMaker session\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "s3_client = boto3.client('s3', region_name=region)\n",
    "athena_client = boto3.client('athena', region_name=region)\n",
    "\n",
    "print(f\"SageMaker Role: {role}\")\n",
    "print(f\"Default S3 Bucket: {bucket}\")\n",
    "print(f\"Region: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98406ff7",
   "metadata": {},
   "source": [
    "### Load and Prepare Data\n",
    "Load dataset.csv and create aggregated + engineered feature tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdfe3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "df_raw = pd.read_csv('dataset.csv')\n",
    "print(f\"Raw data shape: {df_raw.shape}\")\n",
    "print(f\"Raw data columns: {df_raw.columns.tolist()}\")\n",
    "\n",
    "# Aggregate data by timestamp (sum across all locations)\n",
    "df_raw['timestamp'] = pd.to_datetime(df_raw['timestamp'])\n",
    "df_agg = df_raw.groupby('timestamp')['value'].sum().reset_index()\n",
    "df_agg.columns = ['ds', 'y']\n",
    "df_agg = df_agg.sort_values('ds').reset_index(drop=True)\n",
    "df_agg['event_time'] = df_agg['ds']  # For SageMaker Feature Store\n",
    "\n",
    "print(f\"\\nAggregated data shape: {df_agg.shape}\")\n",
    "print(f\"Date range: {df_agg['ds'].min()} to {df_agg['ds'].max()}\")\n",
    "print(df_agg.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31743624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create engineered features\n",
    "def create_features(df):\n",
    "    df = df.copy()\n",
    "    df['month']         = df['ds'].dt.month\n",
    "    df['quarter']       = df['ds'].dt.quarter\n",
    "    df['year']          = df['ds'].dt.year\n",
    "    df['month_sin']     = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos']     = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    df['trend']         = np.arange(len(df))\n",
    "\n",
    "    for lag in [1, 2, 3, 6, 12]:\n",
    "        df[f'lag_{lag}'] = df['y'].shift(lag)\n",
    "\n",
    "    df['rolling_mean_3']  = df['y'].shift(1).rolling(3).mean()\n",
    "    df['rolling_mean_12'] = df['y'].shift(1).rolling(12).mean()\n",
    "    df['rolling_std_3']   = df['y'].shift(1).rolling(3).std()\n",
    "\n",
    "    return df\n",
    "\n",
    "df_feat = create_features(df_agg)\n",
    "df_feat = df_feat.dropna().reset_index(drop=True)\n",
    "df_feat['event_time'] = df_feat['ds']  # For SageMaker Feature Store\n",
    "\n",
    "print(f\"Engineered features shape: {df_feat.shape}\")\n",
    "print(f\"Feature columns: {[c for c in df_feat.columns if c not in ['ds', 'y']]}\")\n",
    "print(df_feat.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2aa7b1",
   "metadata": {},
   "source": [
    "### Upload Data to S3 Data Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dde8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define S3 paths\n",
    "s3_prefix = 'timeseries-demand-forecasting'\n",
    "s3_agg_path = f's3://{bucket}/{s3_prefix}/data-lake/aggregated/'\n",
    "s3_feat_path = f's3://{bucket}/{s3_prefix}/data-lake/features/'\n",
    "s3_staging_dir = f's3://{bucket}/{s3_prefix}/athena/staging'\n",
    "\n",
    "# Convert to Parquet for better performance\n",
    "df_agg.to_parquet('df_agg.parquet', index=False)\n",
    "df_feat.to_parquet('df_feat.parquet', index=False)\n",
    "\n",
    "# Upload to S3\n",
    "sess.upload_data('df_agg.parquet', bucket=bucket, key_prefix=f'{s3_prefix}/data-lake/aggregated/')\n",
    "sess.upload_data('df_feat.parquet', bucket=bucket, key_prefix=f'{s3_prefix}/data-lake/features/')\n",
    "\n",
    "print(f\"✓ Data uploaded to S3\")\n",
    "print(f\"  Aggregated: {s3_agg_path}\")\n",
    "print(f\"  Features:   {s3_feat_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141eff30",
   "metadata": {},
   "source": [
    "### Create Athena Database & Tables\n",
    "\n",
    "Use Athena to query data in the data lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fac1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyathena import connect\n",
    "\n",
    "# Setup Athena connection\n",
    "database_name = \"demand_forecasting\"\n",
    "table_agg = \"timeseries_aggregated\"\n",
    "table_feat = \"timeseries_features\"\n",
    "\n",
    "conn = connect(region_name=region, s3_staging_dir=s3_staging_dir)\n",
    "\n",
    "# Create database\n",
    "create_db = f\"CREATE DATABASE IF NOT EXISTS {database_name}\"\n",
    "pd.read_sql(create_db, conn)\n",
    "print(f\"✓ Database '{database_name}' created\")\n",
    "\n",
    "# Create aggregated table\n",
    "create_agg_table = f\"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS {database_name}.{table_agg} (\n",
    "  ds timestamp,\n",
    "  y double,\n",
    "  event_time timestamp\n",
    ")\n",
    "STORED AS PARQUET\n",
    "LOCATION '{s3_agg_path}'\n",
    "\"\"\"\n",
    "pd.read_sql(create_agg_table, conn)\n",
    "print(f\"✓ Table '{table_agg}' created\")\n",
    "\n",
    "# Create features table\n",
    "create_feat_table = f\"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS {database_name}.{table_feat} (\n",
    "  ds timestamp,\n",
    "  y double,\n",
    "  event_time timestamp,\n",
    "  month int,\n",
    "  quarter int,\n",
    "  year int,\n",
    "  month_sin double,\n",
    "  month_cos double,\n",
    "  trend int,\n",
    "  lag_1 double,\n",
    "  lag_2 double,\n",
    "  lag_3 double,\n",
    "  lag_6 double,\n",
    "  lag_12 double,\n",
    "  rolling_mean_3 double,\n",
    "  rolling_mean_12 double,\n",
    "  rolling_std_3 double\n",
    ")\n",
    "STORED AS PARQUET\n",
    "LOCATION '{s3_feat_path}'\n",
    "\"\"\"\n",
    "pd.read_sql(create_feat_table, conn)\n",
    "print(f\"✓ Table '{table_feat}' created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47b34b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify tables with queries\n",
    "query_agg = f\"SELECT COUNT(*) as row_count FROM {database_name}.{table_agg}\"\n",
    "df_agg_verify = pd.read_sql(query_agg, conn)\n",
    "print(f\"Aggregated table rows: {df_agg_verify['row_count'].values[0]}\")\n",
    "\n",
    "query_feat = f\"SELECT COUNT(*) as row_count FROM {database_name}.{table_feat}\"\n",
    "df_feat_verify = pd.read_sql(query_feat, conn)\n",
    "print(f\"Features table rows: {df_feat_verify['row_count'].values[0]}\")\n",
    "\n",
    "# Sample query\n",
    "sample_query = f\"SELECT ds, y FROM {database_name}.{table_agg} LIMIT 5\"\n",
    "df_sample = pd.read_sql(sample_query, conn)\n",
    "print(f\"\\nSample data from aggregated table:\")\n",
    "print(df_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf23e20",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: SageMaker Feature Store Setup\n",
    "\n",
    "Create feature groups for aggregated data and engineered features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0967687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "from sagemaker.feature_store.inputs import FeatureGroupInputFormat, FeatureGroupOutputFormat\n",
    "\n",
    "# Initialize Feature Store client\n",
    "fs_client = boto3.client('sagemaker-featurestore-runtime', region_name=region)\n",
    "sm_client = boto3.client('sagemaker', region_name=region)\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity()['Account']\n",
    "\n",
    "print(f\"Feature Store initialized for account {account_id}\")\n",
    "print(f\"Region: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cb70dc",
   "metadata": {},
   "source": [
    "### Create Feature Group: Aggregated Time Series Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05d49fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for aggregated feature group\n",
    "fg_agg_name = 'timeseries-aggregated-fg'\n",
    "df_agg['event_time'] = df_agg['event_time'].astype('int64') // 10**9  # Convert to Unix timestamp\n",
    "\n",
    "# Create Feature Group\n",
    "fg_agg = FeatureGroup(\n",
    "    name=fg_agg_name,\n",
    "    sagemaker_session=sess\n",
    ")\n",
    "\n",
    "print(f\"Creating feature group: {fg_agg_name}\")\n",
    "\n",
    "try:\n",
    "    fg_agg.load()\n",
    "    print(f\"✓ Feature group '{fg_agg_name}' already exists\")\n",
    "except:\n",
    "    # Create feature group with Parquet as input format\n",
    "    fg_agg.ingest(\n",
    "        data_frame=df_agg,\n",
    "        max_workers=3,\n",
    "        wait=True\n",
    "    )\n",
    "    print(f\"✓ Feature group '{fg_agg_name}' created and data ingested\")\n",
    "    \n",
    "    # Wait for feature group to be active\n",
    "    fg_agg.wait()\n",
    "    print(f\"✓ Feature group is active\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9a9ba2",
   "metadata": {},
   "source": [
    "### Create Feature Group: Engineered Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c54213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for features feature group\n",
    "fg_feat_name = 'timeseries-engineered-features-fg'\n",
    "df_feat['event_time'] = df_feat['event_time'].astype('int64') // 10**9  # Convert to Unix timestamp\n",
    "\n",
    "# Create Feature Group\n",
    "fg_feat = FeatureGroup(\n",
    "    name=fg_feat_name,\n",
    "    sagemaker_session=sess\n",
    ")\n",
    "\n",
    "print(f\"Creating feature group: {fg_feat_name}\")\n",
    "\n",
    "try:\n",
    "    fg_feat.load()\n",
    "    print(f\"✓ Feature group '{fg_feat_name}' already exists\")\n",
    "except:\n",
    "    # Create feature group\n",
    "    fg_feat.ingest(\n",
    "        data_frame=df_feat,\n",
    "        max_workers=3,\n",
    "        wait=True\n",
    "    )\n",
    "    print(f\"✓ Feature group '{fg_feat_name}' created and data ingested\")\n",
    "    \n",
    "    # Wait for feature group to be active\n",
    "    fg_feat.wait()\n",
    "    print(f\"✓ Feature group is active\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7eaaf5",
   "metadata": {},
   "source": [
    "### Verify Feature Store Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ee8c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List feature groups\n",
    "fgs = sm_client.list_feature_groups(\n",
    "    MaxResults=10,\n",
    "    SortBy='CreationTime',\n",
    "    SortOrder='Descending'\n",
    ")\n",
    "\n",
    "print(\"Feature Groups in SageMaker Feature Store:\")\n",
    "print(\"=\" * 60)\n",
    "for fg in fgs.get('FeatureGroupSummaries', []):\n",
    "    if 'timeseries' in fg['FeatureGroupName']:\n",
    "        print(f\"  • {fg['FeatureGroupName']}\")\n",
    "        print(f\"    Status: {fg['FeatureGroupStatus']}\")\n",
    "        print(f\"    Created: {fg['CreationTime']}\")\n",
    "        print()\n",
    "\n",
    "# Show feature group descriptions\n",
    "print(\"\\nFeature Group Details:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    agg_desc = sm_client.describe_feature_group(FeatureGroupName=fg_agg_name)\n",
    "    print(f\"\\n{fg_agg_name}:\")\n",
    "    print(f\"  Records ingested: {agg_desc.get('RecordIdentifierFeatureGroupArn', 'N/A')}\")\n",
    "    print(f\"  Features: {len(agg_desc.get('FeatureDefinitions', []))}\")\n",
    "    print(f\"  Features: {[f['FeatureName'] for f in agg_desc.get('FeatureDefinitions', [])][:5]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"  Error: {e}\")\n",
    "\n",
    "try:\n",
    "    feat_desc = sm_client.describe_feature_group(FeatureGroupName=fg_feat_name)\n",
    "    print(f\"\\n{fg_feat_name}:\")\n",
    "    print(f\"  Records ingested: {feat_desc.get('RecordIdentifierFeatureGroupArn', 'N/A')}\")\n",
    "    print(f\"  Features: {len(feat_desc.get('FeatureDefinitions', []))}\")\n",
    "    print(f\"  Features: {[f['FeatureName'] for f in feat_desc.get('FeatureDefinitions', [])][:5]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"  Error: {e}\")\n",
    "\n",
    "print(\"\\n✓ Feature Store setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3566d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Using Feature Store with Models\n",
    "\n",
    "Query the feature store to retrieve data for training your forecasting models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6c83ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Query features from Feature Store for model training\n",
    "# This retrieves the latest version of features for each record\n",
    "\n",
    "print(\"Example: Retrieving training data from Feature Store\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load features from Feature Store using Athena (default)\n",
    "df_training = fg_feat.athena_query().as_dataframe()\n",
    "\n",
    "print(f\"✓ Retrieved {len(df_training)} records from feature store\")\n",
    "print(f\"  Shape: {df_training.shape}\")\n",
    "print(f\"  Columns: {df_training.columns.tolist()}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample features:\")\n",
    "print(df_training.head())\n",
    "\n",
    "# Export feature information for documentation\n",
    "feature_store_info = {\n",
    "    'database': database_name,\n",
    "    'tables': {\n",
    "        'aggregated': table_agg,\n",
    "        'features': table_feat\n",
    "    },\n",
    "    'feature_groups': {\n",
    "        'aggregated': fg_agg_name,\n",
    "        'engineered': fg_feat_name\n",
    "    },\n",
    "    's3_paths': {\n",
    "        'data_lake_prefix': s3_prefix,\n",
    "        'aggregated_data': s3_agg_path,\n",
    "        'feature_data': s3_feat_path,\n",
    "        'staging_dir': s3_staging_dir\n",
    "    },\n",
    "    'docs': 'Use these resources to fetch training data for forecasting models'\n",
    "}\n",
    "\n",
    "print(\"\\n✓ Feature Store Configuration Summary:\")\n",
    "import json\n",
    "print(json.dumps(feature_store_info, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f34919",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
