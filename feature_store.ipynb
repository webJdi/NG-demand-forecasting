{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b01abac",
   "metadata": {},
   "source": [
    "# AWS SageMaker Data Lake and Feature Store for Time Series Forecasting\n",
    "\n",
    "This notebook creates:\n",
    "1. **Data Lake** - S3 buckets + Athena tables for raw aggregated time series data\n",
    "2. **Feature Store** - SageMaker Feature Store for engineered features\n",
    "\n",
    "This supports the demand forecasting models built in Modeling.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4102d13f",
   "metadata": {},
   "source": [
    "## Part 1: Data Lake Setup\n",
    "\n",
    "Initialize SageMaker SDK and configure AWS resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72b265a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Initialize SageMaker session\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "s3_client = boto3.client('s3', region_name=region)\n",
    "athena_client = boto3.client('athena', region_name=region)\n",
    "\n",
    "print(f\"SageMaker Role: {role}\")\n",
    "print(f\"Default S3 Bucket: {bucket}\")\n",
    "print(f\"Region: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98406ff7",
   "metadata": {},
   "source": [
    "### Load and Prepare Data\n",
    "Load dataset.csv and create aggregated + engineered feature tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdfe3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "df_raw = pd.read_csv('dataset.csv')\n",
    "print(f\"Raw data shape: {df_raw.shape}\")\n",
    "print(f\"Raw data columns: {df_raw.columns.tolist()}\")\n",
    "\n",
    "# Aggregate data by timestamp (sum across all locations)\n",
    "df_raw['timestamp'] = pd.to_datetime(df_raw['timestamp'])\n",
    "df_agg = df_raw.groupby('timestamp')['value'].sum().reset_index()\n",
    "df_agg.columns = ['ds', 'y']\n",
    "df_agg = df_agg.sort_values('ds').reset_index(drop=True)\n",
    "df_agg['event_time'] = df_agg['ds']  # For SageMaker Feature Store\n",
    "\n",
    "print(f\"\\nAggregated data shape: {df_agg.shape}\")\n",
    "print(f\"Date range: {df_agg['ds'].min()} to {df_agg['ds'].max()}\")\n",
    "print(df_agg.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31743624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create engineered features\n",
    "def create_features(df):\n",
    "    df = df.copy()\n",
    "    df['month']         = df['ds'].dt.month\n",
    "    df['quarter']       = df['ds'].dt.quarter\n",
    "    df['year']          = df['ds'].dt.year\n",
    "    df['month_sin']     = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos']     = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    df['trend']         = np.arange(len(df))\n",
    "\n",
    "    for lag in [1, 2, 3, 6, 12]:\n",
    "        df[f'lag_{lag}'] = df['y'].shift(lag)\n",
    "\n",
    "    df['rolling_mean_3']  = df['y'].shift(1).rolling(3).mean()\n",
    "    df['rolling_mean_12'] = df['y'].shift(1).rolling(12).mean()\n",
    "    df['rolling_std_3']   = df['y'].shift(1).rolling(3).std()\n",
    "\n",
    "    return df\n",
    "\n",
    "df_feat = create_features(df_agg)\n",
    "df_feat = df_feat.dropna().reset_index(drop=True)\n",
    "df_feat['event_time'] = df_feat['ds']  # For SageMaker Feature Store\n",
    "\n",
    "print(f\"Engineered features shape: {df_feat.shape}\")\n",
    "print(f\"Feature columns: {[c for c in df_feat.columns if c not in ['ds', 'y']]}\")\n",
    "print(df_feat.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2aa7b1",
   "metadata": {},
   "source": [
    "### Upload Data to S3 Data Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dde8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define S3 paths\n",
    "s3_prefix = 'timeseries-demand-forecasting'\n",
    "s3_agg_path = f's3://{bucket}/{s3_prefix}/data-lake/aggregated/'\n",
    "s3_feat_path = f's3://{bucket}/{s3_prefix}/data-lake/features/'\n",
    "s3_staging_dir = f's3://{bucket}/{s3_prefix}/athena/staging'\n",
    "\n",
    "# Convert to Parquet for better performance\n",
    "df_agg.to_parquet('df_agg.parquet', index=False)\n",
    "df_feat.to_parquet('df_feat.parquet', index=False)\n",
    "\n",
    "# Upload to S3\n",
    "sess.upload_data('df_agg.parquet', bucket=bucket, key_prefix=f'{s3_prefix}/data-lake/aggregated/')\n",
    "sess.upload_data('df_feat.parquet', bucket=bucket, key_prefix=f'{s3_prefix}/data-lake/features/')\n",
    "\n",
    "print(f\"✓ Data uploaded to S3\")\n",
    "print(f\"  Aggregated: {s3_agg_path}\")\n",
    "print(f\"  Features:   {s3_feat_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141eff30",
   "metadata": {},
   "source": [
    "### Create Athena Database & Tables\n",
    "\n",
    "Use Athena to query data in the data lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fac1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyathena import connect\n",
    "\n",
    "# Setup Athena connection\n",
    "database_name = \"demand_forecasting\"\n",
    "table_agg = \"timeseries_aggregated\"\n",
    "table_feat = \"timeseries_features\"\n",
    "\n",
    "conn = connect(region_name=region, s3_staging_dir=s3_staging_dir)\n",
    "\n",
    "# Create database\n",
    "create_db = f\"CREATE DATABASE IF NOT EXISTS {database_name}\"\n",
    "pd.read_sql(create_db, conn)\n",
    "print(f\"✓ Database '{database_name}' created\")\n",
    "\n",
    "# Create aggregated table\n",
    "create_agg_table = f\"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS {database_name}.{table_agg} (\n",
    "  ds timestamp,\n",
    "  y double,\n",
    "  event_time timestamp\n",
    ")\n",
    "STORED AS PARQUET\n",
    "LOCATION '{s3_agg_path}'\n",
    "\"\"\"\n",
    "pd.read_sql(create_agg_table, conn)\n",
    "print(f\"✓ Table '{table_agg}' created\")\n",
    "\n",
    "# Create features table\n",
    "create_feat_table = f\"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS {database_name}.{table_feat} (\n",
    "  ds timestamp,\n",
    "  y double,\n",
    "  event_time timestamp,\n",
    "  month int,\n",
    "  quarter int,\n",
    "  year int,\n",
    "  month_sin double,\n",
    "  month_cos double,\n",
    "  trend int,\n",
    "  lag_1 double,\n",
    "  lag_2 double,\n",
    "  lag_3 double,\n",
    "  lag_6 double,\n",
    "  lag_12 double,\n",
    "  rolling_mean_3 double,\n",
    "  rolling_mean_12 double,\n",
    "  rolling_std_3 double\n",
    ")\n",
    "STORED AS PARQUET\n",
    "LOCATION '{s3_feat_path}'\n",
    "\"\"\"\n",
    "pd.read_sql(create_feat_table, conn)\n",
    "print(f\"✓ Table '{table_feat}' created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47b34b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify tables with queries\n",
    "query_agg = f\"SELECT COUNT(*) as row_count FROM {database_name}.{table_agg}\"\n",
    "df_agg_verify = pd.read_sql(query_agg, conn)\n",
    "print(f\"Aggregated table rows: {df_agg_verify['row_count'].values[0]}\")\n",
    "\n",
    "query_feat = f\"SELECT COUNT(*) as row_count FROM {database_name}.{table_feat}\"\n",
    "df_feat_verify = pd.read_sql(query_feat, conn)\n",
    "print(f\"Features table rows: {df_feat_verify['row_count'].values[0]}\")\n",
    "\n",
    "# Sample query\n",
    "sample_query = f\"SELECT ds, y FROM {database_name}.{table_agg} LIMIT 5\"\n",
    "df_sample = pd.read_sql(sample_query, conn)\n",
    "print(f\"\\nSample data from aggregated table:\")\n",
    "print(df_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf23e20",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: SageMaker Feature Store Setup\n",
    "\n",
    "Create feature groups for aggregated data and engineered features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0967687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "import time\n",
    "\n",
    "# Initialize SageMaker Feature Store\n",
    "print(f\"Feature Store initialized for region: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cb70dc",
   "metadata": {},
   "source": [
    "### Create Feature Group: Aggregated Time Series Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05d49fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for aggregated feature group\n",
    "fg_agg_name = 'timeseries-aggregated-fg'\n",
    "df_agg['event_time'] = pd.Timestamp.now().timestamp()\n",
    "\n",
    "# Create Feature Group object\n",
    "fg_agg = FeatureGroup(\n",
    "    name=fg_agg_name,\n",
    "    sagemaker_session=sess\n",
    ")\n",
    "\n",
    "# Load feature definitions from the dataframe\n",
    "fg_agg.load_feature_definitions(data_frame=df_agg)\n",
    "\n",
    "print(f\"Creating feature group: {fg_agg_name}\")\n",
    "\n",
    "# Delete existing feature group if it exists\n",
    "try:\n",
    "    fg_agg.delete()\n",
    "    print(\"Found existing feature group. Deleting and waiting 15s for cleanup...\")\n",
    "    time.sleep(15)\n",
    "except Exception:\n",
    "    print(\"No existing feature group found. Proceeding to create.\")\n",
    "\n",
    "# Create feature group with proper parameters\n",
    "try:\n",
    "    fg_agg.create(\n",
    "        s3_uri=f\"s3://{bucket}/{s3_prefix}/data-lake/aggregated/\",\n",
    "        record_identifier_name=\"ds\",\n",
    "        event_time_feature_name=\"event_time\",\n",
    "        role_arn=role,\n",
    "        enable_online_store=False\n",
    "    )\n",
    "    \n",
    "    # Wait for creation to complete\n",
    "    status = fg_agg.describe().get(\"FeatureGroupStatus\")\n",
    "    while status == \"Creating\":\n",
    "        print(\"Waiting for feature group creation...\")\n",
    "        time.sleep(5)\n",
    "        status = fg_agg.describe().get(\"FeatureGroupStatus\")\n",
    "    \n",
    "    print(f\"✓ Feature group '{fg_agg_name}' created successfully!\")\n",
    "    \n",
    "    # Ingest data after creation\n",
    "    print(\"Ingesting data...\")\n",
    "    fg_agg.ingest(\n",
    "        data_frame=df_agg,\n",
    "        max_workers=3,\n",
    "        wait=True\n",
    "    )\n",
    "    print(f\"✓ Data ingested to feature group '{fg_agg_name}'\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating feature group: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9a9ba2",
   "metadata": {},
   "source": [
    "### Create Feature Group: Engineered Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c54213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for engineered features feature group\n",
    "fg_feat_name = 'timeseries-engineered-features-fg'\n",
    "df_feat['event_time'] = pd.Timestamp.now().timestamp()\n",
    "\n",
    "# Create Feature Group object\n",
    "fg_feat = FeatureGroup(\n",
    "    name=fg_feat_name,\n",
    "    sagemaker_session=sess\n",
    ")\n",
    "\n",
    "# Load feature definitions from the dataframe\n",
    "fg_feat.load_feature_definitions(data_frame=df_feat)\n",
    "\n",
    "print(f\"Creating feature group: {fg_feat_name}\")\n",
    "\n",
    "# Delete existing feature group if it exists\n",
    "try:\n",
    "    fg_feat.delete()\n",
    "    print(\"Found existing feature group. Deleting and waiting 15s for cleanup...\")\n",
    "    time.sleep(15)\n",
    "except Exception:\n",
    "    print(\"No existing feature group found. Proceeding to create.\")\n",
    "\n",
    "# Create feature group with proper parameters\n",
    "try:\n",
    "    fg_feat.create(\n",
    "        s3_uri=f\"s3://{bucket}/{s3_prefix}/data-lake/features/\",\n",
    "        record_identifier_name=\"ds\",\n",
    "        event_time_feature_name=\"event_time\",\n",
    "        role_arn=role,\n",
    "        enable_online_store=False\n",
    "    )\n",
    "    \n",
    "    # Wait for creation to complete\n",
    "    status = fg_feat.describe().get(\"FeatureGroupStatus\")\n",
    "    while status == \"Creating\":\n",
    "        print(\"Waiting for feature group creation...\")\n",
    "        time.sleep(5)\n",
    "        status = fg_feat.describe().get(\"FeatureGroupStatus\")\n",
    "    \n",
    "    print(f\"✓ Feature group '{fg_feat_name}' created successfully!\")\n",
    "    \n",
    "    # Ingest data after creation\n",
    "    print(\"Ingesting data...\")\n",
    "    fg_feat.ingest(\n",
    "        data_frame=df_feat,\n",
    "        max_workers=3,\n",
    "        wait=True\n",
    "    )\n",
    "    print(f\"✓ Data ingested to feature group '{fg_feat_name}'\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating feature group: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7eaaf5",
   "metadata": {},
   "source": [
    "### Verify Feature Store Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ee8c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify feature groups were created and check their status\n",
    "print(\"\\nVerifying Feature Groups...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    agg_desc = fg_agg.describe()\n",
    "    agg_status = agg_desc.get(\"FeatureGroupStatus\")\n",
    "    print(f\"\\n{fg_agg_name}:\")\n",
    "    print(f\"  Status: {agg_status}\")\n",
    "    print(f\"  Record Identifier: ds\")\n",
    "    print(f\"  Event Time Feature: event_time\")\n",
    "    print(f\"  Total rows in dataframe: {len(df_agg)}\")\n",
    "except Exception as e:\n",
    "    print(f\"  Error describing aggregated feature group: {e}\")\n",
    "\n",
    "try:\n",
    "    feat_desc = fg_feat.describe()\n",
    "    feat_status = feat_desc.get(\"FeatureGroupStatus\")\n",
    "    print(f\"\\n{fg_feat_name}:\")\n",
    "    print(f\"  Status: {feat_status}\")\n",
    "    print(f\"  Record Identifier: ds\")\n",
    "    print(f\"  Event Time Feature: event_time\")\n",
    "    print(f\"  Total rows in dataframe: {len(df_feat)}\")\n",
    "except Exception as e:\n",
    "    print(f\"  Error describing engineered features group: {e}\")\n",
    "\n",
    "print(\"\\n✓ Feature Store setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3566d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Using Feature Store with Models\n",
    "\n",
    "Query the feature store to retrieve data for training your forecasting models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6c83ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Query features from Feature Store using Athena (Offline Store)\n",
    "print(\"\\nExample: Querying Features from Feature Store (Offline Store)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Get Athena query object from engineered features group\n",
    "    query = fg_feat.athena_query()\n",
    "    table_name = query.table_name\n",
    "    print(f\"Athena Table Name: {table_name}\")\n",
    "    \n",
    "    # Create a SQL query to retrieve sample features\n",
    "    sql_query = f\"\"\"\n",
    "    SELECT ds, y, month, quarter, year, trend, lag_1, lag_12, rolling_mean_3, rolling_mean_12\n",
    "    FROM \"{table_name}\" \n",
    "    ORDER BY ds DESC\n",
    "    LIMIT 10\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nRunning SQL query on Offline Store...\")\n",
    "    \n",
    "    # Execute the query\n",
    "    query.run(\n",
    "        query_string=sql_query,\n",
    "        output_location=f\"s3://{bucket}/{s3_prefix}/query_results\"\n",
    "    )\n",
    "    query.wait()\n",
    "    \n",
    "    # Get results as dataframe\n",
    "    df_results = query.as_dataframe()\n",
    "    \n",
    "    if df_results.empty:\n",
    "        print(\"\\nQuery returned 0 rows.\")\n",
    "    else:\n",
    "        print(f\"\\n✅ Retrieved {len(df_results)} records from Feature Store:\")\n",
    "        print(df_results.head(10))\n",
    "        print(f\"\\n✓ Feature Store is ready for training your forecasting models!\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\nError querying Feature Store: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f34919",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
