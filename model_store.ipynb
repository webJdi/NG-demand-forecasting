{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd7f38b2",
   "metadata": {},
   "source": [
    "# AWS SageMaker Model Store: Time Series Demand Forecasting\n",
    "\n",
    "This notebook creates a model store for the time series forecasting models trained in `Modeling.ipynb`.\n",
    "\n",
    "It demonstrates:\n",
    "1. **Model Package Group** - Container for versioned models\n",
    "2. **Model Packages** - Individual model versions with inference specifications\n",
    "3. **Model Cards** - Metadata and documentation for each model\n",
    "4. **Model Registry** - Track trained models across versions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ade524",
   "metadata": {},
   "source": [
    "## Setup & Initialize SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5671a38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from io import BytesIO\n",
    "\n",
    "# Initialize SageMaker session\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "s3_client = boto3.client('s3', region_name=region)\n",
    "sm_client = boto3.client('sagemaker', region_name=region)\n",
    "\n",
    "print(f\"SageMaker Role: {role}\")\n",
    "print(f\"Default S3 Bucket: {bucket}\")\n",
    "print(f\"Region: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f981d8c8",
   "metadata": {},
   "source": [
    "## Part 1: Load & Serialize Trained Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278db9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and re-train models (or load from disk if available)\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load and prepare data\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df_agg = df.groupby('timestamp')['value'].sum().reset_index()\n",
    "df_agg.columns = ['ds', 'y']\n",
    "df_agg = df_agg.sort_values('ds').reset_index(drop=True)\n",
    "\n",
    "print(f\"Data shape: {df_agg.shape}\")\n",
    "print(f\"Date range: {df_agg['ds'].min()} to {df_agg['ds'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16edb880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering function\n",
    "def create_features(df):\n",
    "    df = df.copy()\n",
    "    df['month']         = df['ds'].dt.month\n",
    "    df['quarter']       = df['ds'].dt.quarter\n",
    "    df['year']          = df['ds'].dt.year\n",
    "    df['month_sin']     = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos']     = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    df['trend']         = np.arange(len(df))\n",
    "\n",
    "    for lag in [1, 2, 3, 6, 12]:\n",
    "        df[f'lag_{lag}'] = df['y'].shift(lag)\n",
    "\n",
    "    df['rolling_mean_3']  = df['y'].shift(1).rolling(3).mean()\n",
    "    df['rolling_mean_12'] = df['y'].shift(1).rolling(12).mean()\n",
    "    df['rolling_std_3']   = df['y'].shift(1).rolling(3).std()\n",
    "\n",
    "    return df\n",
    "\n",
    "df_feat = create_features(df_agg)\n",
    "df_feat = df_feat.dropna().reset_index(drop=True)\n",
    "\n",
    "print(f\"Feature engineered data shape: {df_feat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e457946b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test split\n",
    "TEST_MONTHS = 12\n",
    "\n",
    "feature_cols = [c for c in df_feat.columns if c not in ['ds', 'y']]\n",
    "\n",
    "train = df_feat.iloc[:-TEST_MONTHS]\n",
    "test  = df_feat.iloc[-TEST_MONTHS:]\n",
    "\n",
    "X_train, y_train = train[feature_cols], train['y']\n",
    "X_test,  y_test  = test[feature_cols],  test['y']\n",
    "\n",
    "train_ts = df_agg.iloc[:-TEST_MONTHS]['y']\n",
    "test_ts  = df_agg.iloc[-TEST_MONTHS:]['y']\n",
    "\n",
    "print(f\"Train samples: {len(X_train)}, Test samples: {len(X_test)}\")\n",
    "print(f\"Features: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0747da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models\n",
    "models_dict = {}\n",
    "results_list = []\n",
    "\n",
    "def evaluate(name, y_true, y_pred):\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2   = r2_score(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((np.array(y_true) - np.array(y_pred)) / np.array(y_true))) * 100\n",
    "    return {'Model': name, 'MAE': mae, 'RMSE': rmse, 'R2': r2, 'MAPE': mape}\n",
    "\n",
    "# 1. Linear Regression\n",
    "scaler = StandardScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train)\n",
    "X_test_sc  = scaler.transform(X_test)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_sc, y_train)\n",
    "lr_pred = lr.predict(X_test_sc)\n",
    "models_dict['Linear Regression'] = (lr, scaler, 'StandardScaler')\n",
    "results_list.append(evaluate(\"Linear Regression\", y_test, lr_pred))\n",
    "print(\"‚úì Linear Regression trained\")\n",
    "\n",
    "# 2. Ridge Regression\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train_sc, y_train)\n",
    "ridge_pred = ridge.predict(X_test_sc)\n",
    "models_dict['Ridge Regression'] = (ridge, scaler, 'StandardScaler')\n",
    "results_list.append(evaluate(\"Ridge Regression\", y_test, ridge_pred))\n",
    "print(\"‚úì Ridge Regression trained\")\n",
    "\n",
    "# 3. Random Forest\n",
    "rf = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict(X_test)\n",
    "models_dict['Random Forest'] = (rf, None, None)\n",
    "results_list.append(evaluate(\"Random Forest\", y_test, rf_pred))\n",
    "print(\"‚úì Random Forest trained\")\n",
    "\n",
    "# 4. XGBoost\n",
    "xgb = XGBRegressor(n_estimators=300, learning_rate=0.05, max_depth=4, \n",
    "                   subsample=0.8, colsample_bytree=0.8, random_state=42, verbosity=0)\n",
    "xgb.fit(X_train, y_train)\n",
    "xgb_pred = xgb.predict(X_test)\n",
    "models_dict['XGBoost'] = (xgb, None, None)\n",
    "results_list.append(evaluate(\"XGBoost\", y_test, xgb_pred))\n",
    "print(\"‚úì XGBoost trained\")\n",
    "\n",
    "# 5. LightGBM\n",
    "lgbm = LGBMRegressor(n_estimators=300, learning_rate=0.05, max_depth=4,\n",
    "                     subsample=0.8, colsample_bytree=0.8, random_state=42, verbose=-1)\n",
    "lgbm.fit(X_train, y_train)\n",
    "lgbm_pred = lgbm.predict(X_test)\n",
    "models_dict['LightGBM'] = (lgbm, None, None)\n",
    "results_list.append(evaluate(\"LightGBM\", y_test, lgbm_pred))\n",
    "print(\"‚úì LightGBM trained\")\n",
    "\n",
    "# 6. ETS (Exponential Smoothing)\n",
    "ets_model = ExponentialSmoothing(train_ts, trend='add', seasonal='add', seasonal_periods=12).fit(optimized=True)\n",
    "ets_pred = ets_model.forecast(TEST_MONTHS)\n",
    "models_dict['ETS'] = (ets_model, None, None)\n",
    "results_list.append(evaluate(\"ETS (Holt-Winters)\", test_ts.values, ets_pred.values))\n",
    "print(\"‚úì ETS (Holt-Winters) trained\")\n",
    "\n",
    "# 7. SARIMA\n",
    "sarima_model = SARIMAX(train_ts, order=(1, 1, 1), seasonal_order=(1, 1, 1, 12),\n",
    "                        enforce_stationarity=False, enforce_invertibility=False).fit(disp=False)\n",
    "sarima_pred = sarima_model.forecast(TEST_MONTHS)\n",
    "models_dict['SARIMA'] = (sarima_model, None, None)\n",
    "results_list.append(evaluate(\"SARIMA(1,1,1)(1,1,1,12)\", test_ts, sarima_pred))\n",
    "print(\"‚úì SARIMA trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3171ab63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models\n",
    "results_df = pd.DataFrame(results_list).sort_values('RMSE').reset_index(drop=True)\n",
    "print(\"\\n=== Model Performance (sorted by RMSE) ===\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb60c2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models to S3\n",
    "s3_prefix = 'timeseries-demand-forecasting/models'\n",
    "\n",
    "model_metadata = {}\n",
    "\n",
    "for model_name, (model_obj, scaler_obj, scaler_type) in models_dict.items():\n",
    "    # Serialize model\n",
    "    model_bytes = pickle.dumps(model_obj)\n",
    "    \n",
    "    # Save to S3\n",
    "    s3_key = f\"{s3_prefix}/{model_name.replace(' ', '_').replace('(', '').replace(')', '')}.pkl\"\n",
    "    s3_client.put_object(Bucket=bucket, Key=s3_key, Body=model_bytes)\n",
    "    \n",
    "    # Save scaler if exists\n",
    "    if scaler_obj is not None:\n",
    "        scaler_bytes = pickle.dumps(scaler_obj)\n",
    "        scaler_key = f\"{s3_prefix}/{model_name.replace(' ', '_').replace('(', '').replace(')', '')}_scaler.pkl\"\n",
    "        s3_client.put_object(Bucket=bucket, Key=scaler_key, Body=scaler_bytes)\n",
    "        model_metadata[model_name] = {\n",
    "            'model_s3_path': f\"s3://{bucket}/{s3_key}\",\n",
    "            'scaler_s3_path': f\"s3://{bucket}/{scaler_key}\",\n",
    "            'scaler_type': scaler_type\n",
    "        }\n",
    "    else:\n",
    "        model_metadata[model_name] = {\n",
    "            'model_s3_path': f\"s3://{bucket}/{s3_key}\",\n",
    "            'scaler_s3_path': None,\n",
    "            'scaler_type': None\n",
    "        }\n",
    "\n",
    "print(f\"‚úì All models saved to S3: s3://{bucket}/{s3_prefix}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104095cb",
   "metadata": {},
   "source": [
    "## Part 2: Create Model Package Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0cb8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model Package Group\n",
    "model_package_group_name = \"timeseries-demand-forecasting\"\n",
    "model_package_group_description = \"Time Series Demand Forecasting Models - NG Demand Forecasting\"\n",
    "\n",
    "try:\n",
    "    sm_client.create_model_package_group(\n",
    "        ModelPackageGroupName=model_package_group_name,\n",
    "        ModelPackageGroupDescription=model_package_group_description\n",
    "    )\n",
    "    print(f\"‚úì Model Package Group '{model_package_group_name}' created successfully.\")\n",
    "except Exception as e:\n",
    "    if \"already exists\" in str(e):\n",
    "        print(f\"‚úì Model Package Group '{model_package_group_name}' already exists.\")\n",
    "    else:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Describe the group\n",
    "response = sm_client.describe_model_package_group(\n",
    "    ModelPackageGroupName=model_package_group_name\n",
    ")\n",
    "print(f\"\\nModel Package Group Status: {response['ModelPackageGroupStatus']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f0188b",
   "metadata": {},
   "source": [
    "## Part 3: Create Model Packages for Each Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ca3bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model metadata for each model type\n",
    "model_configs = {\n",
    "    'Linear Regression': {\n",
    "        'description': 'Linear Regression baseline model for time series demand forecasting',\n",
    "        'algorithm': 'Linear Regression',\n",
    "        'framework': 'scikit-learn',\n",
    "        'content_types': ['text/csv', 'application/octet-stream'],\n",
    "        'response_types': ['text/csv']\n",
    "    },\n",
    "    'Ridge Regression': {\n",
    "        'description': 'Ridge Regression with L2 regularization for time series forecasting',\n",
    "        'algorithm': 'Ridge Regression',\n",
    "        'framework': 'scikit-learn',\n",
    "        'content_types': ['text/csv', 'application/octet-stream'],\n",
    "        'response_types': ['text/csv']\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'description': 'Random Forest ensemble model with 200 trees',\n",
    "        'algorithm': 'Random Forest',\n",
    "        'framework': 'scikit-learn',\n",
    "        'content_types': ['text/csv', 'application/octet-stream'],\n",
    "        'response_types': ['text/csv']\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'description': 'XGBoost gradient boosting model optimized for time series',\n",
    "        'algorithm': 'XGBoost',\n",
    "        'framework': 'XGBoost',\n",
    "        'content_types': ['text/csv', 'application/octet-stream'],\n",
    "        'response_types': ['text/csv']\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'description': 'LightGBM light gradient boosting machine model',\n",
    "        'algorithm': 'LightGBM',\n",
    "        'framework': 'LightGBM',\n",
    "        'content_types': ['text/csv', 'application/octet-stream'],\n",
    "        'response_types': ['text/csv']\n",
    "    },\n",
    "    'ETS': {\n",
    "        'description': 'Exponential Smoothing (Holt-Winters) for seasonal time series',\n",
    "        'algorithm': 'Exponential Smoothing',\n",
    "        'framework': 'statsmodels',\n",
    "        'content_types': ['text/csv'],\n",
    "        'response_types': ['text/csv']\n",
    "    },\n",
    "    'SARIMA': {\n",
    "        'description': 'SARIMA(1,1,1)(1,1,1,12) statistical model with seasonal components',\n",
    "        'algorithm': 'SARIMA',\n",
    "        'framework': 'statsmodels',\n",
    "        'content_types': ['text/csv'],\n",
    "        'response_types': ['text/csv']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Get model performance data\n",
    "performance_dict = {row['Model']: row for _, row in results_df.iterrows()}\n",
    "\n",
    "# Create model packages\n",
    "model_package_arns = {}\n",
    "\n",
    "for model_name, config in model_configs.items():\n",
    "    # For this example, we'll create metadata-only packages\n",
    "    # In production, include full inference container specifications\n",
    "    \n",
    "    perf_data = performance_dict.get(model_name, {})\n",
    "    \n",
    "    # Format description as single line (AWS validation regex doesn't allow newlines)\n",
    "    mae_val = f\"{perf_data.get('MAE', 0):.2f}\" if isinstance(perf_data.get('MAE'), (int, float)) else \"N/A\"\n",
    "    rmse_val = f\"{perf_data.get('RMSE', 0):.2f}\" if isinstance(perf_data.get('RMSE'), (int, float)) else \"N/A\"\n",
    "    r2_val = f\"{perf_data.get('R2', 0):.4f}\" if isinstance(perf_data.get('R2'), (int, float)) else \"N/A\"\n",
    "    mape_val = f\"{perf_data.get('MAPE', 0):.2f}\" if isinstance(perf_data.get('MAPE'), (int, float)) else \"N/A\"\n",
    "    \n",
    "    description = f\"{config['description']} | Performance: MAE={mae_val}, RMSE={rmse_val}, R2={r2_val}, MAPE={mape_val}%\"\n",
    "    \n",
    "    try:\n",
    "        response = sm_client.create_model_package(\n",
    "            ModelPackageGroupName=model_package_group_name,\n",
    "            ModelPackageDescription=description,\n",
    "            ModelApprovalStatus='PendingManualApproval',\n",
    "            InferenceSpecification={\n",
    "                'Containers': [\n",
    "                    {\n",
    "                        'Image': 'placeholder-for-inference-image',  # In production, use actual image URI\n",
    "                        'ModelDataUrl': model_metadata[model_name]['model_s3_path'],\n",
    "                        'Framework': config['framework']\n",
    "                    }\n",
    "                ],\n",
    "                'SupportedContentTypes': config['content_types'],\n",
    "                'SupportedResponseMIMETypes': config['response_types']\n",
    "            }\n",
    "        )\n",
    "        model_package_arn = response['ModelPackageArn']\n",
    "        model_package_arns[model_name] = model_package_arn\n",
    "        print(f\"‚úì Model Package created for {model_name}\")\n",
    "        print(f\"  ARN: {model_package_arn}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error creating package for {model_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2f3e65",
   "metadata": {},
   "source": [
    "## Part 4: Create Model Cards with Detailed Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b41a4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "# Create Model Cards for each model\n",
    "for model_name, config in model_configs.items():\n",
    "    perf_data = performance_dict.get(model_name, {})\n",
    "    \n",
    "    model_card_name = f\"timeseries-fc-{model_name.replace(' ', '-').lower()}-{timestamp}\"\n",
    "    \n",
    "    # Model Card content must follow AWS SageMaker's strict schema\n",
    "    card_content = json.dumps({\n",
    "        \"model_overview\": {\n",
    "            \"model_id\": model_name,\n",
    "            \"model_name\": f\"Time Series {model_name}\",\n",
    "            \"model_owner\": \"NG Demand Forecasting Team\",\n",
    "            \"model_version\": 1,\n",
    "            \"problem_type\": \"Time Series Forecasting\",\n",
    "            \"algorithm_type\": config['algorithm']\n",
    "        },\n",
    "        \"intended_uses\": {\n",
    "            \"purpose_of_model\": \"Forecast natural gas demand for the next 12 months\",\n",
    "            \"intended_uses\": \"Production forecasting for demand planning and supply chain optimization\",\n",
    "            \"factors_affecting_model_efficiency\": \"Model performance depends on data quality, seasonal patterns, and external factors like weather.\"\n",
    "        },\n",
    "        \"business_details\": {\n",
    "            \"business_problem\": \"Accurate demand forecasting for supply chain optimization\",\n",
    "            \"business_stakeholders\": \"Operations, Supply Chain, and Finance teams\"\n",
    "        },\n",
    "        \"training_details\": {\n",
    "            \"objective\": \"Minimize RMSE for time series demand forecasting\"\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    try:\n",
    "        sm_client.create_model_card(\n",
    "            ModelCardName=model_card_name,\n",
    "            ModelCardStatus='Draft',\n",
    "            Content=card_content\n",
    "        )\n",
    "        print(f\"‚úì Model Card created for {model_name}\")\n",
    "        print(f\"  Card Name: {model_card_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error creating model card for {model_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7aa945e",
   "metadata": {},
   "source": [
    "## Part 5: Model Registry Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90257063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary report\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL REGISTRY SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüì¶ Model Package Group: {model_package_group_name}\")\n",
    "print(f\"üìç S3 Location: s3://{bucket}/{s3_prefix}/\")\n",
    "print(f\"‚è∞ Created: {timestamp}\")\n",
    "\n",
    "print(\"\\nüìã REGISTERED MODELS:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for idx, row in results_df.iterrows():\n",
    "    model_name = row['Model']\n",
    "    rank = idx + 1\n",
    "    rmse = row['RMSE']\n",
    "    mape = row['MAPE']\n",
    "    \n",
    "    status = \"üèÜ BEST\" if rank == 1 else f\"  #{rank}\"\n",
    "    \n",
    "    print(f\"\\n{status}: {model_name}\")\n",
    "    print(f\"   ARN: {model_package_arns.get(model_name, 'N/A')}\")\n",
    "    print(f\"   RMSE: {rmse:,.1f} | MAPE: {mape:.2f}% | R¬≤: {row['R2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9aa017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model registry to CSV for tracking\n",
    "registry_export = results_df.copy()\n",
    "registry_export['ARN'] = registry_export['Model'].map(model_package_arns)\n",
    "registry_export['Timestamp'] = timestamp\n",
    "registry_export['S3_Path'] = registry_export['Model'].apply(\n",
    "    lambda x: f\"s3://{bucket}/{s3_prefix}/{x.replace(' ', '_').replace('(', '').replace(')', '')}.pkl\"\n",
    ")\n",
    "\n",
    "registry_export.to_csv('model_registry.csv', index=False)\n",
    "print(\"\\n‚úì Model registry exported to model_registry.csv\")\n",
    "print(registry_export[['Model', 'RMSE', 'MAPE', 'R2']].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8e7e58",
   "metadata": {},
   "source": [
    "## Part 6: Approve Best Model for Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024abf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model's ARN\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_model_arn = model_package_arns.get(best_model_name)\n",
    "\n",
    "if best_model_arn:\n",
    "    try:\n",
    "        sm_client.update_model_package(\n",
    "            ModelPackageArn=best_model_arn,\n",
    "            ModelApprovalStatus='Approved'\n",
    "        )\n",
    "        print(f\"‚úì Best Model '{best_model_name}' approved for production\")\n",
    "        print(f\"  ARN: {best_model_arn}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Note: Could not update approval status: {e}\")\n",
    "        print(f\"  Manual approval may be required in SageMaker console\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e869ccd6",
   "metadata": {},
   "source": [
    "## Part 7: Query Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42ac64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all model packages in the group\n",
    "print(f\"\\nüìö Querying Model Registry for: {model_package_group_name}\\n\")\n",
    "\n",
    "paginator = sm_client.get_paginator('list_model_packages')\n",
    "page_iterator = paginator.paginate(\n",
    "    ModelPackageGroupName=model_package_group_name,\n",
    "    PaginationConfig={'PageSize': 10}\n",
    ")\n",
    "\n",
    "model_package_list = []\n",
    "for page in page_iterator:\n",
    "    for package in page['ModelPackageSummaryList']:\n",
    "        model_package_list.append({\n",
    "            'Name': package['ModelPackageName'],\n",
    "            'ARN': package['ModelPackageArn'],\n",
    "            'Status': package['ModelPackageStatus'],\n",
    "            'Approval Status': package.get('ModelApprovalStatus', 'Unknown'),\n",
    "            'Created': package['CreationTime']\n",
    "        })\n",
    "\n",
    "print(f\"Found {len(model_package_list)} model packages:\\n\")\n",
    "for idx, pkg in enumerate(model_package_list, 1):\n",
    "    print(f\"{idx}. {pkg['Name']}\")\n",
    "    print(f\"   Status: {pkg['Status']} | Approved: {pkg['Approval Status']}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
