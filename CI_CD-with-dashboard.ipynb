{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3rGMve5_jMR9",
   "metadata": {
    "id": "3rGMve5_jMR9"
   },
   "source": [
    "## Orchestrate Jobs to Train and Evaluate Models with Amazon SageMaker Pipeline\n",
    "\n",
    "This notebook demonstrates an end-to-end SageMaker Pipeline for a Natural Gas demand forecasting project, including preprocessing, model training, evaluation, batch transform, and model registration and conditional execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mbUbR5AXjhhV",
   "metadata": {
    "id": "mbUbR5AXjhhV"
   },
   "source": [
    "**A SageMaker Pipeline**\n",
    "\n",
    "The pipeline that you create follows a typical machine learning (ML) application pattern of preprocessing, training, evaluation, model creation, batch transformation, and model registration. In this notebook, we use Pipelines to build a repeatable workflow for Natural Gas demand forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CcvLYEbUjxqn",
   "metadata": {
    "id": "CcvLYEbUjxqn"
   },
   "source": [
    "**Dataset: Natural Gas Demand Forecasting**\n",
    "\n",
    "This project uses a Natural Gas dataset to build a regression model that predicts gas demand based on historical observations and related explanatory variables.\n",
    "\n",
    "- Problem Type- The task is a supervised regression problem, where the goal is to predict a continuous numerical target variable representing natural gas demand.\n",
    "\n",
    "- Target Variable-The target variable represents:\n",
    "\n",
    "(1) Natural Gas demand (e.g., daily or monthly consumption)\n",
    "\n",
    "(2) Measured as a continuous numeric value\n",
    "\n",
    "(3) Evaluated using regression metrics such as Mean Squared Error (MSE)\n",
    "\n",
    "(4) The pipeline later uses MSE as the primary model quality metric in the Condition step to determine whether the model passes the quality threshold.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb9dd10f-b57e-4180-aa74-a78a97ab3cbf",
   "metadata": {
    "id": "eb9dd10f-b57e-4180-aa74-a78a97ab3cbf",
    "outputId": "2d53e8fc-7c2b-4543-8cf7-3dd4856d32a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sys, time\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "\n",
    "sagemaker_sess = sagemaker.session.Session()\n",
    "region = sagemaker_sess.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "pipeline_session = PipelineSession()\n",
    "default_bucket = sagemaker_sess.default_bucket()\n",
    "model_package_group_name = f\"NaturalGasModelPackage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17e28020-ea0e-4e3a-ac8c-0ac698c0e785",
   "metadata": {
    "id": "17e28020-ea0e-4e3a-ac8c-0ac698c0e785",
    "outputId": "9fe0b9dc-90c4-43d8-fca2-dd4ab3ece875",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data_uri: s3://sagemaker-us-east-1-137225474160/natural-gas/input/data.csv\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "region = sess.boto_region_name\n",
    "\n",
    "local_path = \"data.csv\"\n",
    "\n",
    "base_uri = f\"s3://{bucket}/natural-gas/input\"\n",
    "\n",
    "input_data_uri = sagemaker.s3.S3Uploader.upload(\n",
    "    local_path=local_path,\n",
    "    desired_s3_uri=base_uri,\n",
    ")\n",
    "\n",
    "print(\"input_data_uri:\", input_data_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cbf80cd-f025-4a93-8d48-3f00a75a524b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "from sagemaker.workflow.steps import CacheConfig\n",
    "\n",
    "# Define a cache that expires after 1 hour (PT1H)\n",
    "cache_config = CacheConfig(enable_caching=True, expire_after=\"PT1H\")\n",
    "# Setup Data Capture (Required for CloudWatch to see inference data)\n",
    "data_capture_prefix = \"sagemaker/natural-gas/datacapture\"\n",
    "s3_capture_upload_path = f\"s3://{default_bucket}/{data_capture_prefix}\"\n",
    "\n",
    "data_capture_config = DataCaptureConfig(\n",
    "    enable_capture=True, \n",
    "    sampling_percentage=100, \n",
    "    destination_s3_uri=s3_capture_upload_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Bis3sYJwlQr6",
   "metadata": {
    "id": "Bis3sYJwlQr6"
   },
   "source": [
    "## Define Parameters to Parametrize Pipeline Execution\n",
    "\n",
    "Pipeline parameters allow you to run the same pipeline multiple times with different values (e.g., instance types, data locations, thresholds, approval status).\n",
    "\n",
    "In this project, key parameters include:\n",
    "- Input data location (S3)\n",
    "- Processing / Training instance types and counts\n",
    "- Model approval status (if using Model Registry)\n",
    "- MSE threshold used in the quality gate (Condition step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd820478-9419-46ab-9858-ddb8d6ac2a8d",
   "metadata": {
    "id": "cd820478-9419-46ab-9858-ddb8d6ac2a8d"
   },
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    "    ParameterFloat,\n",
    ")\n",
    "\n",
    "processing_instance_count = ParameterInteger(name=\"ProcessingInstanceCount\", default_value=1)\n",
    "\n",
    "\n",
    "instance_type = ParameterString(name=\"TrainingInstanceType\", default_value=\"ml.m5.xlarge\")\n",
    "\n",
    "model_approval_status = ParameterString(\n",
    "    name=\"ModelApprovalStatus\", default_value=\"PendingManualApproval\"\n",
    ")\n",
    "\n",
    "input_data = ParameterString(\n",
    "    name=\"InputData\",\n",
    "    default_value=input_data_uri,\n",
    ")\n",
    "\n",
    "mse_threshold = ParameterFloat(name=\"MseThreshold\", default_value=10000000.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fca20d09-5f6a-42f6-a641-a9361ec5354f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sagemaker-user/NG-demand-forecasting'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZIP3uI9Hlk6N",
   "metadata": {
    "id": "ZIP3uI9Hlk6N"
   },
   "source": [
    "## Define a Processing Step for Feature Engineering\n",
    "\n",
    "This Processing step performs dataset-specific preprocessing and feature engineering, and writes artifacts back to S3 for downstream steps.\n",
    "\n",
    "Typical outputs:\n",
    "- `train/` (training data)\n",
    "- `test/` (test data)\n",
    "- `batch/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59503265-72df-4ede-9a10-72a985c63a04",
   "metadata": {
    "id": "59503265-72df-4ede-9a10-72a985c63a04",
    "outputId": "d28c1757-4e67-4653-81bd-e977ab6000e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/sagemaker-user/NG-demand-forecasting/code/preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /home/sagemaker-user/NG-demand-forecasting/code/preprocessing.py\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "import argparse\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Input directory/file (SageMaker Processing input)\n",
    "    parser.add_argument(\"--input-dir\", type=str, default=\"/opt/ml/processing/input\")\n",
    "    parser.add_argument(\"--input-file\", type=str, default=\"data.csv\")\n",
    "\n",
    "    # Output directories (SageMaker Processing outputs)\n",
    "    parser.add_argument(\"--train-output\", type=str, default=\"/opt/ml/processing/train\")\n",
    "    parser.add_argument(\"--validation-output\", type=str, default=\"/opt/ml/processing/validation\")\n",
    "    parser.add_argument(\"--test-output\", type=str, default=\"/opt/ml/processing/test\")\n",
    "    parser.add_argument(\"--batch-output\", type=str, default=\"/opt/ml/processing/batch\")\n",
    "\n",
    "    # Separate metadata output (IMPORTANT: keep non-CSV out of train/val/test)\n",
    "    parser.add_argument(\"--metadata-output\", type=str, default=\"/opt/ml/processing/metadata\")\n",
    "\n",
    "    # Column configuration (for your Kaggle natural gas dataset)\n",
    "    parser.add_argument(\"--year-col\", type=str, default=\"year\")\n",
    "    parser.add_argument(\"--month-col\", type=str, default=\"month\")\n",
    "    parser.add_argument(\"--target-col\", type=str, default=\"value\")\n",
    "    parser.add_argument(\"--units-col\", type=str, default=\"units\")\n",
    "\n",
    "    # Slice selection (recommended for time-series lags; otherwise auto-picks most frequent)\n",
    "    parser.add_argument(\"--series-col\", type=str, default=\"series\")\n",
    "    parser.add_argument(\"--series-filter\", type=str, default=\"\")\n",
    "    parser.add_argument(\"--duoarea-col\", type=str, default=\"duoarea\")\n",
    "    parser.add_argument(\"--duoarea-filter\", type=str, default=\"\")\n",
    "\n",
    "    # Feature engineering settings\n",
    "    parser.add_argument(\"--max-lag\", type=int, default=12)\n",
    "    parser.add_argument(\"--roll-window-1\", type=int, default=3)\n",
    "    parser.add_argument(\"--roll-window-2\", type=int, default=12)\n",
    "\n",
    "    # Time splits\n",
    "    parser.add_argument(\"--test-months\", type=int, default=12)\n",
    "    parser.add_argument(\"--val-months\", type=int, default=6)\n",
    "\n",
    "    # Batch transform window (features only). If 0 -> same as test window.\n",
    "    parser.add_argument(\"--batch-months\", type=int, default=0)\n",
    "\n",
    "    # Output formatting for built-in XGBoost (NO header when flag is set)\n",
    "    parser.add_argument(\"--no-header\", action=\"store_true\")\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def ensure_dirs(*dirs):\n",
    "    for d in dirs:\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "\n",
    "\n",
    "def safe_to_numeric(s):\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "\n",
    "def choose_most_frequent_value(df, col):\n",
    "    vc = df[col].value_counts(dropna=True)\n",
    "    if vc.empty:\n",
    "        raise ValueError(f\"Cannot auto-select {col}: no non-null values.\")\n",
    "    return vc.index[0]\n",
    "\n",
    "\n",
    "def add_time_features(df, ds_col=\"ds\"):\n",
    "    df[\"year\"] = df[ds_col].dt.year\n",
    "    df[\"month\"] = df[ds_col].dt.month\n",
    "    df[\"quarter\"] = df[ds_col].dt.quarter\n",
    "    df[\"month_sin\"] = np.sin(2 * np.pi * df[\"month\"] / 12.0)\n",
    "    df[\"month_cos\"] = np.cos(2 * np.pi * df[\"month\"] / 12.0)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_lag_features(df, y_col=\"y\", max_lag=12):\n",
    "    for lag in range(1, max_lag + 1):\n",
    "        df[f\"lag_{lag}\"] = df[y_col].shift(lag)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_rolling_features(df, y_col=\"y\", windows=(3, 12)):\n",
    "    for w in windows:\n",
    "        df[f\"roll_mean_{w}\"] = df[y_col].rolling(window=w).mean()\n",
    "        df[f\"roll_std_{w}\"] = df[y_col].rolling(window=w).std()\n",
    "        df[f\"roll_min_{w}\"] = df[y_col].rolling(window=w).min()\n",
    "        df[f\"roll_max_{w}\"] = df[y_col].rolling(window=w).max()\n",
    "    return df\n",
    "\n",
    "\n",
    "def resolve_input_path(input_dir: str, input_file: str) -> str:\n",
    "    # Prefer explicit file name if present\n",
    "    candidate = os.path.join(input_dir, input_file)\n",
    "    if os.path.exists(candidate):\n",
    "        return candidate\n",
    "\n",
    "    # Otherwise, pick first CSV in directory\n",
    "    csvs = sorted(glob.glob(os.path.join(input_dir, \"*.csv\")))\n",
    "    if not csvs:\n",
    "        raise FileNotFoundError(\n",
    "            f\"No CSV found in {input_dir}. Expected {candidate} or any *.csv file.\"\n",
    "        )\n",
    "    return csvs[0]\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "\n",
    "    # Resolve input path robustly\n",
    "    input_path = resolve_input_path(args.input_dir, args.input_file)\n",
    "    df = pd.read_csv(input_path)\n",
    "\n",
    "    # Validate columns exist\n",
    "    required = {args.year_col, args.month_col, args.target_col, args.series_col, args.duoarea_col}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}. Found: {list(df.columns)}\")\n",
    "\n",
    "    # Basic cleaning\n",
    "    df[args.year_col] = safe_to_numeric(df[args.year_col])\n",
    "    df[args.month_col] = safe_to_numeric(df[args.month_col])\n",
    "    df[args.target_col] = safe_to_numeric(df[args.target_col])\n",
    "\n",
    "    df = df.dropna(subset=[args.year_col, args.month_col, args.target_col]).copy()\n",
    "    df[args.year_col] = df[args.year_col].astype(int)\n",
    "    df[args.month_col] = df[args.month_col].astype(int)\n",
    "    df = df[(df[args.month_col] >= 1) & (df[args.month_col] <= 12)].copy()\n",
    "\n",
    "    # Build ds and y\n",
    "    df[\"ds\"] = pd.to_datetime(\n",
    "        dict(year=df[args.year_col], month=df[args.month_col], day=1),\n",
    "        errors=\"coerce\",\n",
    "    )\n",
    "    df = df.dropna(subset=[\"ds\"]).copy()\n",
    "    df[\"y\"] = df[args.target_col].astype(float)\n",
    "\n",
    "    # Select ONE series slice\n",
    "    if args.series_filter.strip():\n",
    "        df = df[df[args.series_col].astype(str) == args.series_filter.strip()].copy()\n",
    "        selected_series = args.series_filter.strip()\n",
    "    else:\n",
    "        selected_series = choose_most_frequent_value(df, args.series_col)\n",
    "        df = df[df[args.series_col] == selected_series].copy()\n",
    "\n",
    "    if args.duoarea_filter.strip():\n",
    "        df = df[df[args.duoarea_col].astype(str) == args.duoarea_filter.strip()].copy()\n",
    "        selected_duoarea = args.duoarea_filter.strip()\n",
    "    else:\n",
    "        selected_duoarea = choose_most_frequent_value(df, args.duoarea_col)\n",
    "        df = df[df[args.duoarea_col] == selected_duoarea].copy()\n",
    "\n",
    "    # Optionally ensure a single unit (helps consistency)\n",
    "    selected_unit = None\n",
    "    if args.units_col in df.columns:\n",
    "        df[args.units_col] = df[args.units_col].astype(str)\n",
    "        selected_unit = choose_most_frequent_value(df, args.units_col)\n",
    "        df = df[df[args.units_col] == selected_unit].copy()\n",
    "\n",
    "    # Aggregate duplicates by month\n",
    "    df = (\n",
    "        df.groupby(\"ds\", as_index=False)\n",
    "          .agg(y=(\"y\", \"mean\"))\n",
    "          .sort_values(\"ds\")\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # Feature engineering\n",
    "    df_feat = df.copy()\n",
    "    df_feat = add_time_features(df_feat, ds_col=\"ds\")\n",
    "    df_feat = add_lag_features(df_feat, y_col=\"y\", max_lag=args.max_lag)\n",
    "    df_feat = add_rolling_features(df_feat, y_col=\"y\", windows=(args.roll_window_1, args.roll_window_2))\n",
    "\n",
    "    # Drop rows created by lags/rolling\n",
    "    df_feat = df_feat.dropna().reset_index(drop=True)\n",
    "\n",
    "    # Ensure enough rows\n",
    "    test_months = int(args.test_months)\n",
    "    val_months = int(args.val_months)\n",
    "    if df_feat.shape[0] < (test_months + val_months + 5):\n",
    "        raise ValueError(\n",
    "            f\"Not enough rows after feature engineering. Got {df_feat.shape[0]} rows. \"\n",
    "            f\"Need at least {test_months + val_months + 5}. \"\n",
    "            f\"Reduce lags/windows or choose a series with longer history.\"\n",
    "        )\n",
    "\n",
    "    # Split (time-based)\n",
    "    n = df_feat.shape[0]\n",
    "    test_start = n - test_months\n",
    "    val_start = test_start - val_months\n",
    "\n",
    "    train_df = df_feat.iloc[:val_start].copy()\n",
    "    val_df = df_feat.iloc[val_start:test_start].copy()\n",
    "    test_df = df_feat.iloc[test_start:].copy()\n",
    "\n",
    "    # Features (exclude ds and y)\n",
    "    feature_cols = [c for c in df_feat.columns if c not in [\"ds\", \"y\"]]\n",
    "\n",
    "    X_train, y_train = train_df[feature_cols], train_df[\"y\"]\n",
    "    X_val, y_val = val_df[feature_cols], val_df[\"y\"]\n",
    "    X_test, y_test = test_df[feature_cols], test_df[\"y\"]\n",
    "\n",
    "    # Batch (features only)\n",
    "    batch_months = int(args.batch_months) if int(args.batch_months) > 0 else test_months\n",
    "    batch_df = df_feat.iloc[-batch_months:].copy()\n",
    "    X_batch = batch_df[feature_cols].copy()\n",
    "\n",
    "    # Write ONLY CSVs into train/val/test/batch folders\n",
    "    ensure_dirs(args.train_output, args.validation_output, args.test_output, args.batch_output, args.metadata_output)\n",
    "\n",
    "    header = not args.no_header\n",
    "\n",
    "    def write_label_first_csv(out_dir, filename, X, y):\n",
    "        # label first column for built-in XGBoost\n",
    "        out_path = os.path.join(out_dir, filename)\n",
    "        out = pd.concat([y.reset_index(drop=True), X.reset_index(drop=True)], axis=1)\n",
    "        out.to_csv(out_path, index=False, header=header)\n",
    "\n",
    "    def write_features_only_csv(out_dir, filename, X):\n",
    "        out_path = os.path.join(out_dir, filename)\n",
    "        X.to_csv(out_path, index=False, header=header)\n",
    "\n",
    "    write_label_first_csv(args.train_output, \"train.csv\", X_train, y_train)\n",
    "    write_label_first_csv(args.validation_output, \"validation.csv\", X_val, y_val)\n",
    "    write_label_first_csv(args.test_output, \"test.csv\", X_test, y_test)\n",
    "    write_features_only_csv(args.batch_output, \"batch.csv\", X_batch)\n",
    "\n",
    "    # Metadata goes to metadata folder ONLY (keeps XGBoost channels clean)\n",
    "    meta = {\n",
    "        \"input_path\": input_path,\n",
    "        \"selected_series\": selected_series,\n",
    "        \"selected_duoarea\": selected_duoarea,\n",
    "        \"selected_unit\": selected_unit,\n",
    "        \"rows_after_filter\": int(df.shape[0]),\n",
    "        \"rows_after_feat_dropna\": int(df_feat.shape[0]),\n",
    "        \"train_rows\": int(train_df.shape[0]),\n",
    "        \"val_rows\": int(val_df.shape[0]),\n",
    "        \"test_rows\": int(test_df.shape[0]),\n",
    "        \"batch_rows\": int(X_batch.shape[0]),\n",
    "        \"feature_count\": int(len(feature_cols)),\n",
    "        \"max_lag\": int(args.max_lag),\n",
    "        \"roll_windows\": [int(args.roll_window_1), int(args.roll_window_2)],\n",
    "        \"test_months\": int(test_months),\n",
    "        \"val_months\": int(val_months),\n",
    "        \"no_header\": bool(args.no_header),\n",
    "    }\n",
    "\n",
    "    pd.Series(meta).to_json(os.path.join(args.metadata_output, \"preprocess_meta.json\"))\n",
    "    pd.Series(feature_cols).to_csv(\n",
    "        os.path.join(args.metadata_output, \"feature_columns.csv\"),\n",
    "        index=False,\n",
    "        header=[\"feature\"],\n",
    "    )\n",
    "\n",
    "    print(\"Preprocessing complete.\")\n",
    "    print(meta)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Q_31qUrJl69Z",
   "metadata": {
    "id": "Q_31qUrJl69Z"
   },
   "source": [
    "**Creating an instance of a SKLearnProcessor processor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f695dcb-56d9-4d22-8d74-5c198e6198a9",
   "metadata": {
    "id": "5f695dcb-56d9-4d22-8d74-5c198e6198a9",
    "outputId": "3d9b3aa9-96c1-455a-d7e8-2e879de236dd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The input argument instance_type of function (sagemaker.image_uris.retrieve) is a pipeline variable (<class 'sagemaker.workflow.parameters.ParameterString'>), which is interpreted in pipeline execution time only. As the function needs to evaluate the argument value in SDK compile time, the default_value of this Parameter object will be used to override it. Please make sure the default_value is valid.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "framework_version = \"1.2-1\"\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=framework_version,\n",
    "    instance_type=instance_type,                 # <-- change this\n",
    "    instance_count=processing_instance_count,\n",
    "    base_job_name=\"sklearn-natgas-process\",      # optional rename\n",
    "    role=role,\n",
    "    sagemaker_session=pipeline_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24OHROtumF13",
   "metadata": {
    "id": "24OHROtumF13"
   },
   "source": [
    "**Taking the output of the processor's run method and pass that as arguments to the ProcessingStep**\n",
    "\n",
    "Here generated a batch feature file from the latest window inside preprocessing, then exposed it as ProcessingOutput(output_name=\"batch\"), which is exactly why our TransformStep can run without a second source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebd4293c-f5ce-4004-bf58-7cd64fb937c8",
   "metadata": {
    "id": "ebd4293c-f5ce-4004-bf58-7cd64fb937c8",
    "outputId": "b864d5c0-4d7f-402c-a859-c9f2057d2b15"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/sagemaker/workflow/pipeline_context.py:332: UserWarning: Running within a PipelineSession, there will be No Wait, No Logs, and No Job being started.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "processor_args = sklearn_processor.run(\n",
    "    inputs=[\n",
    "        ProcessingInput(source=input_data, destination=\"/opt/ml/processing/input\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\"),\n",
    "        ProcessingOutput(output_name=\"validation\", source=\"/opt/ml/processing/validation\"),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\"),\n",
    "        ProcessingOutput(output_name=\"batch\", source=\"/opt/ml/processing/batch\"),\n",
    "    ],\n",
    "    code=\"/home/sagemaker-user/NG-demand-forecasting/code/preprocessing.py\",\n",
    "    arguments=[\"--no-header\"],                   \n",
    ")\n",
    "\n",
    "step_process = ProcessingStep(name=\"NatGasProcess\", step_args=processor_args, cache_config=cache_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p9l69vsImTv5",
   "metadata": {
    "id": "p9l69vsImTv5"
   },
   "source": [
    "# Define a Training Step to Train a Model\n",
    "\n",
    "This step trains a regression model using the processed training dataset produced by the previous step.\n",
    "\n",
    "The trained model artifacts are saved to S3 (e.g., `model.tar.gz`) and are later consumed by evaluation, CreateModel, and/or RegisterModel steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0a1a743-43e9-49f4-99ef-a66db6031a33",
   "metadata": {
    "id": "c0a1a743-43e9-49f4-99ef-a66db6031a33",
    "outputId": "0d045f83-a041-4a92-9983-6bd0a52946ad"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The input argument instance_type of function (sagemaker.image_uris.retrieve) is a pipeline variable (<class 'sagemaker.workflow.parameters.ParameterString'>), which is interpreted in pipeline execution time only. As the function needs to evaluate the argument value in SDK compile time, the default_value of this Parameter object will be used to override it. Please make sure the default_value is valid.\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "\n",
    "# Store model artifacts here\n",
    "model_path = f\"s3://{default_bucket}/NaturalGasTrain\"\n",
    "\n",
    "# Built-in XGBoost container\n",
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"xgboost\",\n",
    "    region=region,\n",
    "    version=\"1.0-1\",\n",
    "    py_version=\"py3\",\n",
    "    instance_type=instance_type,\n",
    ")\n",
    "\n",
    "# Estimator\n",
    "xgb_train = Estimator(\n",
    "    image_uri=image_uri,\n",
    "    instance_type=instance_type,\n",
    "    instance_count=1,\n",
    "    output_path=model_path,\n",
    "    role=role,\n",
    "    sagemaker_session=pipeline_session,\n",
    ")\n",
    "\n",
    "# Hyperparameters (regression)\n",
    "xgb_train.set_hyperparameters(\n",
    "    objective=\"reg:squarederror\",\n",
    "    num_round=50,\n",
    "    max_depth=5,\n",
    "    eta=0.2,\n",
    "    gamma=4,\n",
    "    min_child_weight=6,\n",
    "    subsample=0.7,\n",
    "\n",
    ")\n",
    "\n",
    "# Fit args (these are pipeline step args, not an immediate training job)\n",
    "train_args = xgb_train.fit(\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "        \"validation\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"validation\"].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Training step\n",
    "step_train = TrainingStep(\n",
    "    name=\"NaturalGasTrain\",\n",
    "    step_args=train_args,\n",
    "    cache_config=cache_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "czMV3jilmm4t",
   "metadata": {
    "id": "czMV3jilmm4t"
   },
   "source": [
    "# Define a Model Evaluation Step to Evaluate the Trained Model\n",
    "\n",
    "This Processing step evaluates the trained model on the test dataset and writes an evaluation report (JSON) to S3.\n",
    "\n",
    "For this project, we track regression metrics such as Mean Squared Error (MSE). The downstream Condition step reads the metric value from the evaluation report and decides whether to proceed or fail the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80cac7b1-caed-40dd-a3ab-4f98bb5eb730",
   "metadata": {
    "id": "80cac7b1-caed-40dd-a3ab-4f98bb5eb730",
    "outputId": "1a966661-4305-4003-bed4-6424e60539ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/sagemaker-user/NG-demand-forecasting/code/evaluation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /home/sagemaker-user/NG-demand-forecasting/code/evaluation.py\n",
    "import json\n",
    "import pathlib\n",
    "import pickle\n",
    "import tarfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Model artifact from TrainingStep\n",
    "    model_path = \"/opt/ml/processing/model/model.tar.gz\"\n",
    "    with tarfile.open(model_path) as tar:\n",
    "        tar.extractall(path=\".\")\n",
    "\n",
    "    # Built-in XGBoost training writes the model file named \"xgboost-model\"\n",
    "    model = pickle.load(open(\"xgboost-model\", \"rb\"))\n",
    "\n",
    "    # Test data from ProcessingStep output: headerless, label-first\n",
    "    test_path = \"/opt/ml/processing/test/test.csv\"\n",
    "    df = pd.read_csv(test_path, header=None)\n",
    "\n",
    "    y_test = df.iloc[:, 0].to_numpy()\n",
    "    X_test = df.iloc[:, 1:].to_numpy()\n",
    "\n",
    "    dtest = xgboost.DMatrix(X_test)\n",
    "    preds = model.predict(dtest)\n",
    "\n",
    "    mse = mean_squared_error(y_test, preds)\n",
    "    std = float(np.std(y_test - preds))\n",
    "\n",
    "    report_dict = {\n",
    "        \"regression_metrics\": {\n",
    "            \"mse\": {\"value\": float(mse), \"standard_deviation\": std},\n",
    "        }\n",
    "    }\n",
    "\n",
    "    output_dir = \"/opt/ml/processing/evaluation\"\n",
    "    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    evaluation_path = f\"{output_dir}/evaluation.json\"\n",
    "    with open(evaluation_path, \"w\") as f:\n",
    "        json.dump(report_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8r7oS0xFmyWc",
   "metadata": {
    "id": "8r7oS0xFmyWc"
   },
   "source": [
    "**Creating an instance of a ScriptProcessor processor and use it in the ProcessingStep**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72ddf908-5402-434b-8d16-af763910816f",
   "metadata": {
    "id": "72ddf908-5402-434b-8d16-af763910816f"
   },
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "script_eval = ScriptProcessor(\n",
    "    image_uri=image_uri,  # reuse the same xgboost image_uri\n",
    "    command=[\"python3\"],\n",
    "    instance_type=instance_type,   # <-- use the same variable/parameter used elsewhere\n",
    "    instance_count=1,\n",
    "    base_job_name=\"script-natgas-eval\",\n",
    "    role=role,\n",
    "    sagemaker_session=pipeline_session,\n",
    ")\n",
    "\n",
    "eval_args = script_eval.run(\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination=\"/opt/ml/processing/model\",\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=step_process.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri,\n",
    "            destination=\"/opt/ml/processing/test\",\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\"),\n",
    "    ],\n",
    "    code=\"/home/sagemaker-user/NG-demand-forecasting/code/evaluation.py\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ly-fRsaLnAPH",
   "metadata": {
    "id": "ly-fRsaLnAPH"
   },
   "source": [
    "**Using the processor's arguments returned by .run() to construct a ProcessingStep, along with the input and output channels and the code that will be executed when the pipeline invokes pipeline execution**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "087f4e01-135a-4187-a8a0-7f0bedd9a68d",
   "metadata": {
    "id": "087f4e01-135a-4187-a8a0-7f0bedd9a68d"
   },
   "outputs": [],
   "source": [
    "from sagemaker.workflow.properties import PropertyFile\n",
    "\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"EvaluationReport\",\n",
    "    output_name=\"evaluation\",\n",
    "    path=\"evaluation.json\",\n",
    ")\n",
    "\n",
    "step_eval = ProcessingStep(\n",
    "    name=\"NaturalGasEval\",\n",
    "    step_args=eval_args,\n",
    "    property_files=[evaluation_report],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fQ6JyhynIFm",
   "metadata": {
    "id": "8fQ6JyhynIFm"
   },
   "source": [
    "# Define a Create Model Step to Create a Model\n",
    "\n",
    "This step creates a SageMaker Model object from the training artifacts (model.tar.gz) and the inference container image. This model can be used for Batch Transform (offline inference) or endpoint deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9b5b179-7775-4df4-b836-27f75114fe7f",
   "metadata": {
    "id": "b9b5b179-7775-4df4-b836-27f75114fe7f"
   },
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "\n",
    "model = Model(\n",
    "    image_uri=image_uri,\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=pipeline_session,\n",
    "    role=role,\n",
    "    name=f\"natural-gas-model-{int(time.time())}\"\n",
    ")\n",
    "\n",
    "step_create_model = ModelStep(\n",
    "    name=\"NaturalGasCreateModel\",\n",
    "    step_args=model.create(\n",
    "        instance_type=instance_type,   # or use transform_instance_type if you have it\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-qIb7pN7nVzN",
   "metadata": {
    "id": "-qIb7pN7nVzN"
   },
   "source": [
    "# Define a Transform Step to Perform Batch Transformation\n",
    "\n",
    "Batch Transform runs offline inference on a dataset stored in S3 (for example, a batch input created during preprocessing).\n",
    "\n",
    "This is useful when you want predictions on large datasets without hosting a persistent endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f74a521-9ece-4909-84ed-da1cccc7be4f",
   "metadata": {
    "id": "6f74a521-9ece-4909-84ed-da1cccc7be4f"
   },
   "outputs": [],
   "source": [
    "from sagemaker.transformer import Transformer\n",
    "from sagemaker.inputs import TransformInput\n",
    "from sagemaker.workflow.steps import TransformStep\n",
    "\n",
    "transformer = Transformer(\n",
    "    model_name=step_create_model.properties.ModelName,\n",
    "    instance_type=instance_type,  # or use transform_instance_type if you have a separate parameter\n",
    "    instance_count=1,\n",
    "    output_path=f\"s3://{default_bucket}/NaturalGasTransform\",\n",
    "    assemble_with=\"Line\",\n",
    ")\n",
    "\n",
    "step_transform = TransformStep(\n",
    "    name=\"NaturalGasTransform\",\n",
    "    transformer=transformer,\n",
    "    inputs=TransformInput(\n",
    "        data=step_process.properties.ProcessingOutputConfig.Outputs[\"batch\"].S3Output.S3Uri,\n",
    "        content_type=\"text/csv\",\n",
    "        split_type=\"Line\", # standard for CSV\n",
    "\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hffpDiaBnsXa",
   "metadata": {
    "id": "hffpDiaBnsXa"
   },
   "source": [
    "# Define a Register Model Step to Create a Model Package\n",
    "\n",
    "A model package groups all artifacts needed for inference (model data + inference image + metadata). Model packages can be versioned in a Model Package Group, enabling you to track and approve models over time.\n",
    "\n",
    "If enabled, we register the trained model as a new version in the projectâ€™s Model Package Group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43f9babf-b536-4db2-a690-19076c8b5349",
   "metadata": {
    "id": "43f9babf-b536-4db2-a690-19076c8b5349",
    "outputId": "dc673e06-f1c8-41b0-a53e-1b0ccad05c01"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "\n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=\"{}/evaluation.json\".format(\n",
    "            step_eval.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "        ),\n",
    "        content_type=\"application/json\",\n",
    "    )\n",
    ")\n",
    "\n",
    "register_args = model.register(\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[\"ml.t2.medium\", \"ml.m5.large\"],   # keep simple / commonly available\n",
    "    transform_instances=[\"ml.m5.large\", \"ml.m5.xlarge\"],  # keep what your lab allows\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    approval_status=model_approval_status,\n",
    "    model_metrics=model_metrics,\n",
    ")\n",
    "\n",
    "step_register = ModelStep(\n",
    "    name=\"NaturalGasRegisterModel\",\n",
    "    step_args=register_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xFhmeYK9n2E_",
   "metadata": {
    "id": "xFhmeYK9n2E_"
   },
   "source": [
    "# Define a Fail Step to Terminate the Pipeline Execution and Mark it as Failed\n",
    "\n",
    "This step reads the evaluation report and checks whether the model meets a quality threshold (e.g., MSE <= threshold).\n",
    "\n",
    "- If the condition passes: proceed to CreateModel + Batch Transform (+ optional RegisterModel)\n",
    "- If the condition fails: execute a Fail step with a clear error message so the pipeline run is marked as Failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05e9bdaf-6487-4dbf-9869-fd266924ac35",
   "metadata": {
    "id": "05e9bdaf-6487-4dbf-9869-fd266924ac35"
   },
   "outputs": [],
   "source": [
    "from sagemaker.workflow.fail_step import FailStep\n",
    "from sagemaker.workflow.functions import Join\n",
    "\n",
    "step_fail = FailStep(\n",
    "    name=\"NaturalGasMSEFail\",\n",
    "    error_message=Join(on=\" \", values=[\"Execution failed due to MSE >\", mse_threshold]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "233a1486-a16a-482e-9e56-5fd5f25b5681",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.model_monitor import ModelQualityMonitor\n",
    "from sagemaker import image_uris\n",
    "\n",
    "# Retrieve the model monitor image\n",
    "monitor_image_uri = image_uris.retrieve(framework=\"model-monitor\", region=region)\n",
    "\n",
    "model_quality_monitor = ModelQualityMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=1800,\n",
    "    sagemaker_session=pipeline_session\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_97Smv31n8G1",
   "metadata": {
    "id": "_97Smv31n8G1"
   },
   "source": [
    "# Define a Condition Step to Check Accuracy and Conditionally Create a Model and Run a Batch Transformation and Register a Model in the Model Registry, Or Terminate the Execution in Failed State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f39ed3c-8a9e-4ae0-a733-ebb942f52e7e",
   "metadata": {
    "id": "1f39ed3c-8a9e-4ae0-a733-ebb942f52e7e"
   },
   "outputs": [],
   "source": [
    "from sagemaker.workflow.conditions import ConditionLessThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "\n",
    "cond_lte = ConditionLessThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step_name=step_eval.name,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"regression_metrics.mse.value\",\n",
    "    ),\n",
    "    right=mse_threshold,\n",
    ")\n",
    "\n",
    "step_cond = ConditionStep(\n",
    "    name=\"NaturalGasMSECond\",\n",
    "    conditions=[cond_lte],\n",
    "    if_steps=[step_register, step_create_model, step_transform],\n",
    "    else_steps=[step_fail],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9gxTEEBpoLzp",
   "metadata": {
    "id": "9gxTEEBpoLzp"
   },
   "source": [
    "# Define a Pipeline of Parameters, Steps, and Conditions\n",
    "\n",
    "In this section we combine:\n",
    "- Parameters\n",
    "- Processing / Training / Evaluation steps\n",
    "- Condition + Fail logic\n",
    "into a single Pipeline DAG that can be created/updated and executed in SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8abd12f-1b67-4199-a441-4b7bc95bc624",
   "metadata": {
    "id": "d8abd12f-1b67-4199-a441-4b7bc95bc624"
   },
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline_name = \"NaturalGasPipeline\"\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_count,\n",
    "        instance_type,\n",
    "        model_approval_status,\n",
    "        input_data,\n",
    "        mse_threshold,\n",
    "    ],\n",
    "    steps=[step_process, step_train, step_eval, step_cond],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OqVLc3_3oYXZ",
   "metadata": {
    "id": "OqVLc3_3oYXZ"
   },
   "source": [
    "**Submit the pipeline to SageMaker and start execution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab284580-71ff-4d74-a9c1-d7fda3482f23",
   "metadata": {
    "id": "ab284580-71ff-4d74-a9c1-d7fda3482f23",
    "outputId": "1c154a9a-b230-4272-9bc2-8a7ce23a2fbd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow._utils:Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TransformJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TransformJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:137225474160:pipeline/NaturalGasPipeline',\n",
       " 'ResponseMetadata': {'RequestId': '64569090-28c6-4931-a522-e2a834f89dc2',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '64569090-28c6-4931-a522-e2a834f89dc2',\n",
       "   'strict-transport-security': 'max-age=47304000; includeSubDomains',\n",
       "   'x-frame-options': 'DENY',\n",
       "   'content-security-policy': \"frame-ancestors 'none'\",\n",
       "   'cache-control': 'no-cache, no-store, must-revalidate',\n",
       "   'x-content-type-options': 'nosniff',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '108',\n",
       "   'date': 'Mon, 23 Feb 2026 16:50:35 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54b26a8c-68b9-44cb-8351-4aafbd55ca2c",
   "metadata": {
    "id": "54b26a8c-68b9-44cb-8351-4aafbd55ca2c"
   },
   "outputs": [],
   "source": [
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "T1NDAQ5Mof2N",
   "metadata": {
    "id": "T1NDAQ5Mof2N"
   },
   "source": [
    "**Pipeline Operations: Examining and Waiting for Pipeline Execution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "600ab43a-4ae9-4822-8833-d28d0849f491",
   "metadata": {
    "id": "600ab43a-4ae9-4822-8833-d28d0849f491",
    "outputId": "38b225af-80fd-4c45-a343-c99b4dbf3a9a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:137225474160:pipeline/NaturalGasPipeline',\n",
       " 'PipelineExecutionArn': 'arn:aws:sagemaker:us-east-1:137225474160:pipeline/NaturalGasPipeline/execution/yrc71tnivyr0',\n",
       " 'PipelineExecutionDisplayName': 'execution-1771865435902',\n",
       " 'PipelineExecutionStatus': 'Executing',\n",
       " 'CreationTime': datetime.datetime(2026, 2, 23, 16, 50, 35, 846000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2026, 2, 23, 16, 50, 35, 846000, tzinfo=tzlocal()),\n",
       " 'CreatedBy': {'UserProfileArn': 'arn:aws:sagemaker:us-east-1:137225474160:user-profile/d-u2y08ppawvnm/default-1771745007294',\n",
       "  'UserProfileName': 'default-1771745007294',\n",
       "  'DomainId': 'd-u2y08ppawvnm',\n",
       "  'IamIdentity': {'Arn': 'arn:aws:sts::137225474160:assumed-role/LabRole/SageMaker',\n",
       "   'PrincipalId': 'AROAR742GSBYNSR3W5KWG:SageMaker'}},\n",
       " 'LastModifiedBy': {'UserProfileArn': 'arn:aws:sagemaker:us-east-1:137225474160:user-profile/d-u2y08ppawvnm/default-1771745007294',\n",
       "  'UserProfileName': 'default-1771745007294',\n",
       "  'DomainId': 'd-u2y08ppawvnm',\n",
       "  'IamIdentity': {'Arn': 'arn:aws:sts::137225474160:assumed-role/LabRole/SageMaker',\n",
       "   'PrincipalId': 'AROAR742GSBYNSR3W5KWG:SageMaker'}},\n",
       " 'ResponseMetadata': {'RequestId': 'fe225502-65e4-4473-ac39-9f90e8b0248d',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'fe225502-65e4-4473-ac39-9f90e8b0248d',\n",
       "   'strict-transport-security': 'max-age=47304000; includeSubDomains',\n",
       "   'x-frame-options': 'DENY',\n",
       "   'content-security-policy': \"frame-ancestors 'none'\",\n",
       "   'cache-control': 'no-cache, no-store, must-revalidate',\n",
       "   'x-content-type-options': 'nosniff',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '1039',\n",
       "   'date': 'Mon, 23 Feb 2026 16:50:36 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea2f00e-407c-4f0b-8fc0-8c69560dd22c",
   "metadata": {
    "id": "5ea2f00e-407c-4f0b-8fc0-8c69560dd22c"
   },
   "outputs": [],
   "source": [
    "execution.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e06814a-abe7-4678-97b9-18037034b358",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_endpoint_name = f\"gas-endpoint-{int(time.time())}\"\n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    data_capture_config=data_capture_config,\n",
    "    endpoint_name=unique_endpoint_name\n",
    ")\n",
    "\n",
    "endpoint_name = predictor.endpoint_name\n",
    "print(f\"Endpoint deployed: {endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729597a7-4aef-487d-94f2-57a036880b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the Failed Job's Arn\n",
    "from sagemaker.workflow.selective_execution_config import SelectiveExecutionConfig\n",
    "\n",
    "failed_execution_arn = execution.arn\n",
    "failed_execution_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4383d8e9-e0e2-4ef6-a920-f34fdd6b4476",
   "metadata": {},
   "outputs": [],
   "source": [
    "selective_execution_config = SelectiveExecutionConfig(\n",
    "    source_pipeline_execution_arn=failed_execution_arn,\n",
    "    selected_steps=[\"NaturalGasEval\", \"NaturalGasCond\", \"NaturalGasCreateModel\"] # Add the create step here!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2a5c83-443e-4df3-879e-b78f26472303",
   "metadata": {},
   "outputs": [],
   "source": [
    "selective_execution = pipeline.start(\n",
    "    selective_execution_config=selective_execution_config,\n",
    "    parameters={\"MseThreshold\": 400000.0}\n",
    ")\n",
    "\n",
    "print(f\"Selective Execution started: {selective_execution.arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd689896-4d71-44f3-8572-c80bde323f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "selective_execution.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7d1dfc-42d9-48a1-a45d-ff6b90ba2786",
   "metadata": {
    "id": "3f7d1dfc-42d9-48a1-a45d-ff6b90ba2786",
    "outputId": "7ca808f4-e562-4b9f-ded0-9a064868bfb0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "execution.list_steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e92376-7caf-472e-b50f-4de5d030ea0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "selective_execution.list_steps()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KVZ2bUkAoqVg",
   "metadata": {
    "id": "KVZ2bUkAoqVg"
   },
   "source": [
    "**Examining the Evaluation**\n",
    "\n",
    "Examine the resulting model evaluation after the pipeline completes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd1a0b3-a9bf-4e61-be66-6d6ce692accc",
   "metadata": {
    "id": "2bd1a0b3-a9bf-4e61-be66-6d6ce692accc",
    "outputId": "1deab48d-9303-4045-af0e-dd4f7f9ac0e5"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import sagemaker\n",
    "\n",
    "evaluation_json = sagemaker.s3.S3Downloader.read_file(\n",
    "    \"{}/evaluation.json\".format(\n",
    "        step_eval.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "    )\n",
    ")\n",
    "unique_endpoint_name = f\"gas-endpoint-{int(time.time())}\"\n",
    "pprint(json.loads(evaluation_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46848a7e-8685-41ad-9478-5e1e90ad2798",
   "metadata": {
    "id": "46848a7e-8685-41ad-9478-5e1e90ad2798"
   },
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import EndpointInput\n",
    "\n",
    "# Set the schedule to run every 10 minutes\n",
    "# Note: Use standard cron expression for 10-minute intervals\n",
    "cron_10_min = \"cron(0/10 * * * ? *)\"\n",
    "\n",
    "model_quality_monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=\"NaturalGas-Quality-Monitor-10Min\",\n",
    "    endpoint_input=EndpointInput(\n",
    "        endpoint_name=endpoint_name, # The endpoint name from your CI/CD pipeline\n",
    "        destination=\"/opt/ml/processing/input/endpoint\",\n",
    "    ),\n",
    "    output_s3_uri=f\"s3://{bucket}/sagemaker/natural-gas/monitoring_reports\",\n",
    "    # Reference baseline metrics (ensure these variables are defined or hardcoded)\n",
    "    statistics=s3_baseline_statistics_uri, \n",
    "    constraints=s3_baseline_constraints_uri,\n",
    "    schedule_cron_expression=cron_10_min,\n",
    "    enable_cloudwatch_metrics=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
